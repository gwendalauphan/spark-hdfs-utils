# Spark HDFS Utils

**Spark HDFS Utils** est un ensemble de classes et de fonctions facilitant l‚Äôinteraction entre **Apache Spark** et un cluster **Hadoop HDFS**. Il propose notamment des m√©thodes de traitement de donn√©es, des utilitaires pour manipuler des DataFrames et des fonctionnalit√©s pour g√©rer des chemins et r√©pertoires sous HDFS.

üöÄ **DEMO** ‚Üí [demo video](#demo)

## Table des mati√®res

- [Spark HDFS Utils](#spark-hdfs-utils)
  - [Table des mati√®res](#table-des-mati√®res)
  - [Fonctionnalit√©s](#fonctionnalit√©s)
  - [Tester le package et modifier la classe d‚Äôentr√©e](#tester-le-package-et-modifier-la-classe-dentr√©e)
  - [Utilisation du Toolkit Docker](#utilisation-du-toolkit-docker)
    - [Configuration](#configuration)
    - [Commandes disponibles](#commandes-disponibles)
      - [Exemple d‚Äôutilisation](#exemple-dutilisation)
  - [Tests Unitaires](#tests-unitaires)
- [Demo](#demo)
- [Refs](#refs)
- [Repos source](#repos-source)

---

## Fonctionnalit√©s

Le projet est organis√© autour de trois grands **packages** :

1. **Recette** (fonctions de transformation et de traitement de donn√©es)
   - **fonctionsRecette.InfoFiles**
     - `addNameColumn`
     - `creerIndexDataFrame`
     - `supprimerLignesDataFrame`
     - `getInfoFiles`, `printInfoFiles`
     - `getSizeFiles`, `printSizeFiles`
     - `getRawCountFiles`, `printRawCountFiles`
     - `getColumnFiles`, `printColumnFiles`
     - `writeInfoFiles`

2. **Utils** (fonctions utilitaires pour Spark et fichiers)
   - **fonctionsUtils.UtilsSpark**
     - `creerSparkSession`
     - `creerEtSetBasePardefaut`
   - **fonctionsUtils.UtilsFile**
     - `writeSingleFile`
     - `readFile`
     - `controleColonnesDf`
     - `countLines`
   - **fonctionsUtils.UtilsDev**
     - `compareRegex`
     - `biggestSizeFile`
     - `mostRecentDateFile`
     - `compareDates`

3. **Hdfs** (fonctions utilitaires pour g√©rer HDFS)
   - **fonctionsHdfs.UtilsHdfs**
     - `controlExistenceChemin`
     - `getDirsWithFiles`
     - `listerRepertoireHdfs`

Ces m√©thodes couvrent diff√©rents besoins du d√©veloppement Spark : cr√©ation de **Session Spark**, manipulation de **DataFrames**, interactions avec **HDFS**, et op√©rations vari√©es comme la comparaison de regex, la lecture/√©criture de fichiers, etc.

---

## Tester le package et modifier la classe d‚Äôentr√©e

Afin de **tester le package**, vous pouvez modifier le fichier
`app/src/main/scala/runFonctions/EssaiMain.scala`
et lancer la commande :
```bash
sbt run
```
dans le **dossier racine du projet**.

Si vous souhaitez **changer la classe d‚Äôentr√©e par d√©faut**, √©ditez le fichier `build.sbt` et modifiez la ligne :
```scala
mainClass in (Compile, run) := Some("runFonctions.EssaiMain")
```

---

## Utilisation du Toolkit Docker

Le projet propose un **toolkit** permettant de lancer le code Scala/Spark sans installer localement Java, Scala, SBT ni Spark.

> **Note** : Vous aurez besoin d‚Äôun compte Docker pour acc√©der au Hub Docker et, si n√©cessaire, pousser vos images. Les identifiants sont configur√©s dans le fichier `.env` (non versionn√© par git), au sein du dossier `toolkit`.

### Configuration

1. Dans le dossier `toolkit/`, cr√©ez un fichier `.env` similaire √† :
   ```
    DOCKER_USER=xxxxxxxx
    DOCKER_PASS=xxxxxxxxxxxx
    JAVA_VERSION=11.0.14.1
    SBT_VERSION=1.6.2
    SCALA_VERSION=2.12.15
   ```
   - Ajustez les versions par d√©faut de Java, Scala et SBT si besoin.

2. Rendez le script `sbt.sh` ex√©cutable :
   ```bash
   chmod +x sbt.sh
   ```

### Commandes disponibles

Le script `sbt.sh` propose quatre commandes :

- `bash` : ouvre un terminal bash dans le conteneur
- `run` : ex√©cute `sbt run` (lance la classe main)
- `package` : ex√©cute `sbt package` (compile et g√©n√®re le .jar)
- `test` : ex√©cute les tests unitaires

#### Exemple d‚Äôutilisation

```bash
cd toolkit

./sbt.sh help     # Affiche l'aide
# -->  Usage: ./sbt.sh {bash|run|package|test} [JAVA_VERSION=xxx] [SBT_VERSION=xxx] [SCALA_VERSION=xxx]
./sbt.sh run      # Lance le code Scala
./sbt.sh bash     # Ouvre un terminal bash dans le conteneur
./sbt.sh package  # Compile le projet
./sbt.sh test     # Lance les tests unitaires
```

---

## Tests Unitaires

Si des tests unitaires sont impl√©ment√©s, vous pouvez les lancer via Docker en utilisant le toolkit :

```bash
cd toolkit
./sbt.sh test
```

---

# Demo

[demo.webm](https://github.com/user-attachments/assets/3e81645c-2714-41b0-90a7-5808a18405f5)

---

# Refs

- **Hadoop**
  - [bigdatafoundation/docker-hadoop](https://github.com/bigdatafoundation/docker-hadoop)
  - [Article Medium sur la configuration d‚Äôun cluster HDFS Docker](https://bytemedirk.medium.com/setting-up-an-hdfs-cluster-with-docker-compose-a-step-by-step-guide-4541cd15b168)

- **Spark**
  - [Image Docker Bitnami Spark](https://hub.docker.com/r/bitnami/spark/tags)
  - [Github Bitnami Spark](https://github.com/bitnami/containers/tree/main/bitnami/spark)

- **Scala et SBT**
  - [Docker SBT](https://hub.docker.com/r/sbtscala/scala-sbt)
  - [GitHub Docker SBT](https://github.com/sbt/docker-sbt)

- **Maven Repository**
  - [mvnrepository.com](https://mvnrepository.com/)

# Repos source

Base: https://gitlab.com/informatique9073415/library/scala/spark-hdfs-utils.git

Mirror: https://github.com/gwendalauphan/spark-hdfs-utils.git
